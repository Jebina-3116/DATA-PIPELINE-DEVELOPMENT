# etl_pipeline.py

# üì¶ Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

#Ô∏è EXTRACT: Load the data
def extract_data(file_path):
    print("Extracting data...")
    data = pd.read_csv(file_path)
    print("Data shape:", data.shape)
    return data
#Ô∏è TRANSFORM: Preprocess the data
def transform_data(data):
    print("\nTransforming data...")

    # Identify numeric and categorical columns
    numeric_cols = data.select_dtypes(include=['int64', 'float64']).columns
    categorical_cols = data.select_dtypes(include=['object']).columns

    print("Numeric columns:", list(numeric_cols))
    print("Categorical columns:", list(categorical_cols))

    # Pipelines for numeric and categorical features
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('encoder', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combine transformations
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_cols),
            ('cat', categorical_transformer, categorical_cols)
        ]
    )

    # Fit and transform the data
    transformed_data = preprocessor.fit_transform(data)

    # Return transformed data as DataFrame (optional)
    transformed_df = pd.DataFrame(
        transformed_data.toarray() if hasattr(transformed_data, "toarray") else transformed_data
    )
    
    print("Transformed data shape:", transformed_df.shape)
    return transformed_df
#Ô∏è LOAD: Save the cleaned data to CSV
def load_data(cleaned_data, output_path):
    print("\nLoading data to CSV...")
    cleaned_data.to_csv(output_path, index=False)
    print(f"Cleaned data saved to {output_path}")

#  Main function to run pipeline
def run_pipeline():
    input_file = "sample_data.csv"  # Replace with your CSV file path
    output_file = "cleaned_data.csv"

    # Run the ETL steps
    raw_data = extract_data(input_file)
    cleaned_data = transform_data(raw_data)
    load_data(cleaned_data, output_file)

#  Execute the pipeline
if __name__ == "__main__":
    run_pipeline()

# Create a dummy CSV file for testing
import pandas as pd
import numpy as np

# Generate some sample data
data = {
    'numeric_col_1': np.random.rand(100) * 100,
    'numeric_col_2': np.random.randint(1, 50, 100),
    'categorical_col_1': np.random.choice(['A', 'B', 'C', 'D'], 100),
    'categorical_col_2': np.random.choice(['X', 'Y', 'Z', None], 100, p=[0.3, 0.3, 0.3, 0.1]), # Include some missing values
    'numeric_col_3': np.random.randn(100) * 10 + 50 # Another numeric column
}

sample_df = pd.DataFrame(data)

# Introduce some missing values in numeric columns
for col in ['numeric_col_1', 'numeric_col_2', 'numeric_col_3']:
    sample_df.loc[sample_df.sample(frac=0.05).index, col] = np.nan

# Save the dummy data to a CSV file
sample_df.to_csv("sample_data.csv", index=False)

print("Created sample_data.csv")
display(sample_df.head())
